# Model Configuration
# The ID of the model to be fine-tuned, sourced from Hugging Face.
MODEL_ID="HuggingFaceTB/SmolLM-135M-Instruct"

# Dataset Configuration
# The ID of the dataset to fine-tune the model on (the new skill), sourced from Hugging Face.
DATASET_ID="medalpaca/medical_meadow_medical_flashcards"
INSTRUCTION_COLUMN_NAME="input"
RESPONSE_COLUMN_NAME="output"
# PEFT (Parameter-Efficient Fine-Tuning) Configuration
# r: Rank of the low-rank adaptation matrix.
# lora_alpha: Scaling factor for the adaptation matrix.
# lora_dropout: Dropout probability to prevent overfitting.
# bias: Bias type used in LoRA.
PEFT_R=16
PEFT_LORA_ALPHA=32
PEFT_LORA_DROPOUT=0.05
PEFT_BIAS="none"

# SFT (Supervised Fine-Tuning) Configuration
# Output directory, training parameters, and optimization settings.
# output_dir: Directory where fine-tuned models and logs are saved.
# num_train_epochs: Number of epochs to train the model.
# max_seq_length: Maximum sequence length for the input data.
# per_device_train_batch_size: Batch size per GPU/TPU core.
# gradient_accumulation_steps: Number of updates steps to accumulate before performing a backward/update pass.
# gradient_checkpointing: Whether to use gradient checkpointing to save memory.
# optim: Optimization algorithm used during training.
# save_steps: Number of steps between saving model checkpoints.
# logging_steps: Number of steps between logging metrics.
# learning_rate: Initial learning rate for the optimizer.
# weight_decay: Weight decay to apply to the optimizer.
# fp16: Whether to use 16-bit floating-point precision for training.
# bf16: Whether to use bfloat16 precision for training.
# warmup_ratio: Ratio of steps to warm up the learning rate.
# lr_scheduler_type: Type of learning rate scheduler to use.

SFT_NUM_TRAIN_EPOCHS=10
SFT_MAX_SEQ_LENGTH=512
SFT_PER_DEVICE_TRAIN_BATCH_SIZE=16
SFT_GRADIENT_ACCUMULATION_STEPS=2
SFT_GRADIENT_CHECKPOINTING=true
SFT_OPTIM="paged_adamw_32bit"
SFT_SAVE_STEPS=500
SFT_LOGGING_STEPS=500
SFT_LEARNING_RATE=0.001
SFT_WEIGHT_DECAY=0.001
SFT_FP16=false
SFT_BF16=true
SFT_WARMUP_RATIO=0.05
SFT_LR_SCHEDULER_TYPE="constant"
